# -*- coding: utf-8 -*-
"""
Created on Tue Jul  4 12:05:07 2023

@author: LocalAdmin
No TFNET however the network size is reduced.. used for final sims

2007: Unified for different persistence and features
2607: Works across different combinations
0308 13:30 -- Commented the TNET transform, earlier was not working for single prediction
0308 15:30 -- Single prediction working. Changes done to enable import from a difft program without running it
21/08: Added AHC for clustering
22/08: Correction for noise. Tried adding statisticsal parameters (in a difft program)
"""

# -*- coding: utf-8 -*-
"""PointNetClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/nikitakaraevv/pointnet/blob/master/nbs/PointNetClass.ipynb

# PointNet

This is an implementation of [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) using PyTorch.

## Getting started

**Runtime** -> **Change runtime type**-> **Hardware accelerator**
"""
import numpy as np
import math
import random
import os
import torch
import time
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchinfo import summary
from sklearn.cluster import AgglomerativeClustering
import pandas as pd
import sys
from scipy import stats

#!pip install path.py;
from path import Path
import torch.nn as nn
import torch.nn.functional as F
import itertools
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score

#%% Initialization. Enter the correct persistence and features so that the correct training file and architecture is picked up

random.seed = 42

persistence = 5 # Valid selection 1, 2, 3, 4, 5
features = 4 # Valid selection 2, 3, 4
enable_training = True # Train only if this is true else directly validation

if persistence == 1:
    path = Path(r"Training_labeled/persistence_1")
elif persistence == 2:
    path = Path(r"Training_labeled/persistence_2")
elif persistence == 3:
    path = Path(r"Training_labeled/persistence_3")
elif persistence == 4:
    path = Path(r"Training_labeled/persistence_4")
elif persistence == 5:
    path = Path(r"Training_labeled/persistence_5")
else:
    print("Invalid value of persistence")
    sys.exit()
print("Training folder:",str(path))

# Get the classes from the folder names in the training directory and assign a number to each class for classification
folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/Path(dir))]
# {'0_person': 0,
#  '1_person': 1,
#  '2_person': 2,
#  '3_person': 3,
#  '4_person': 4}
classes = {folder: i for i, folder in enumerate(folders)}
classes

#%%

# Function to standardize the cluster within a pointcloud. Each cluster should have fixed dimensions. Each standardized cluster represents a single input to ML algorithm

class Standardize:
    def __init__(self, file, features, persistence):  # dataframe for a single cluster within a file

        self.paddedsize = 0
        self.clustersize = 0
        self.processed_df = pd.DataFrame()
        self.ptcld = np.array([])

        if persistence == 1:
            self.clustersize = 64
        elif persistence == 2:
            self.clustersize = 128
        elif persistence == 3:
            self.clustersize = 256
        elif persistence == 4:
            self.clustersize = 512
        elif persistence == 5:
            self.clustersize = 1024
        else:
            print("Invalid persistence ... Exiting")
            sys.exit()

        curr_file = Path(file)
        self.df_col_list = ['range', 'azim', 'dopp', 'snr', 'y', 'x', 'current_frame', 'seq']
        if features == 2:
            self.selected_cols_for_csv = ['y', 'x', 'seq']
        elif features == 3:
            self.selected_cols_for_csv = ['y', 'x', 'seq', 'snr']
        elif features == 4:
            self.selected_cols_for_csv = ['y', 'x', 'seq', 'snr', 'dopp']
        else:
            print("Invalid selection for features... Exiting")
            sys.exit()

        if os.path.isfile(curr_file) is False:
            print("Not a valid file")
            print(curr_file)

        curr_df = pd.read_csv(curr_file, names=self.df_col_list)
        if curr_df.shape[0] == 0:
            print("Empty dataframe", Path(curr_file))

        self.curr_df = curr_df
        self.standardize_df()

    def standardize_df(self):

        # Check if the cluster size is less than the required size, if so pad it
        if self.curr_df.shape[0] < self.clustersize:
            self.processed_df = self.pad_df()
        # Check if the cluster size is more than the required size, if so trim it
        elif self.curr_df.shape[0] > self.clustersize:
            self.processed_df = self.trim_df()
        else:
            self.processed_df = self.curr_df

        # Padded size will be positive when padded, negative when trimmed
        self.paddedsize = self.clustersize - self.curr_df.shape[0]
        # Convert the dataframe to numpy array
        self.ptcld = self.processed_df[self.selected_cols_for_csv].to_numpy()

        # Delete the seq column
        self.ptcld = np.delete(self.ptcld,2,1)

        return self.ptcld

    def pad_df(self):

        df_col_list = list(self.curr_df.columns)

        supp_data_size = self.clustersize % self.curr_df.shape[0]
        df_aug = pd.DataFrame(columns=df_col_list)
        processed_df = pd.DataFrame(columns=df_col_list)
        if (supp_data_size != 0):
            cen = np.empty([supp_data_size, 2])
            snr = np.empty(supp_data_size)
            dopp = np.empty(supp_data_size)
            seq = np.empty(supp_data_size)

            col1 = self.curr_df.y.to_numpy()
            col2 = self.curr_df.x.to_numpy()
            col3 = self.curr_df.snr.to_numpy()
            col4 = self.curr_df.dopp.to_numpy()
            col5 = self.curr_df.seq.to_numpy()
            aggloclust = AgglomerativeClustering(n_clusters=supp_data_size).fit(col1.reshape(-1, 1),
                                                                                col2.reshape(-1, 1))
            labels = aggloclust.labels_

            for i in range(0, supp_data_size):
                sel_y = [col1[j] for j in range(len(col1)) if labels[j] == i]
                sel_x = [col2[j] for j in range(len(col2)) if labels[j] == i]
                sel_snr = [col3[j] for j in range(len(col3)) if labels[j] == i]
                sel_dopp = [col4[j] for j in range(len(col4)) if labels[j] == i]
                sel_seq = [col5[j] for j in range(len(col5)) if labels[j] == i]
                cen[i] = np.average([sel_y, sel_x], axis=1, weights=sel_snr)
                snr[i] = np.average(sel_snr)
                dopp[i] = np.average(sel_dopp)
                seq[i] = stats.mode(sel_seq, keepdims=False).mode

            df_aug['y'] = cen[:, 0]
            df_aug['x'] = cen[:, 1]
            df_aug['snr'] = snr
            df_aug['dopp'] = dopp
            df_aug['seq'] = seq

        while self.clustersize - processed_df.shape[0] >= self.curr_df.shape[0]:
            processed_df = processed_df.dropna(axis=1, how='all')
            self.curr_df = self.curr_df.dropna(axis=1, how='all')
            processed_df = pd.concat([processed_df, self.curr_df],
                                     ignore_index=True)  # keep repeating the data integer no of times till possible

        processed_df = processed_df.dropna(axis=1, how='all')
        df_aug = df_aug.dropna(axis=1, how='all')
        processed_df = pd.concat([processed_df, df_aug], ignore_index=True)

        return processed_df

def trim_df(self):
        #df_col_list = list(self.curr_df.columns)
        processed_df = self.curr_df.sort_values('snr',ascending=False).head(self.clustersize)
        processed_df = processed_df.sort_index(axis=0,ascending=True)
        #processed_df=curr_df
        return(processed_df)

#%%

"""
Default transformations from Pointnet. However not using Normalize and RandRotation. Only Random noise has been added

"""

class Normalize(object):
    def __call__(self, pointcloud):
        #print(pointcloud)
        assert len(pointcloud.shape)==2

        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0)
        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))

        return  norm_pointcloud

#norm_pointcloud = Normalize()(pointcloud)

"""Notice that axis limits have changed.

### Augmentations

Let's add *random rotation* of the whole pointcloud and random noise to its points.
"""

class RandRotation_z(object):
    def __call__(self, pointcloud):
        assert len(pointcloud.shape)==2

        theta = random.random() * 2. * math.pi
        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],
                               [ math.sin(theta),  math.cos(theta),    0],
                               [0,                             0,      1]])

        rot_pointcloud = rot_matrix.dot(pointcloud.T).T
        return  rot_pointcloud


class RandomNoise(object):
    def __call__(self, pointcloud):
        assert len(pointcloud.shape) == 2
        
        noisy_pointcloud = pointcloud # initialize with same values and shape
        noise_x = np.random.normal(0, 0.2*np.var(pointcloud[:, 1]), (pointcloud.shape[0])) # x
        noise_y = np.random.normal(0, 0.2*np.var(pointcloud[:, 0]), (pointcloud.shape[0])) # y
        noisy_pointcloud[:, 0] = pointcloud[:, 0] + noise_y
        noisy_pointcloud[:, 1] = pointcloud[:, 1] + noise_x
        return  noisy_pointcloud

"""### ToTensor"""

class ToTensor(object):
    def __call__(self, pointcloud):
        assert len(pointcloud.shape)==2

        return torch.from_numpy(pointcloud)

#ToTensor()(noisy_rot_pointcloud)

def default_transforms():
    return transforms.Compose([
                                ToTensor()
                              ])

"""## Dataset

Now we can create a [custom PyTorch Dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
"""

class PointCloudData(Dataset):
    def __init__(self, root_dir, features, persistence, valid=False, folder="train", transform=default_transforms()):
        self.root_dir = root_dir
        self.features = features
        self.persistence = persistence
        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]
        self.classes = {folder: i for i, folder in enumerate(folders)}
        self.transforms = transform if not valid else default_transforms()
        self.valid = valid
        self.files = []
        for category in self.classes.keys():
            new_dir = root_dir/Path(category)/folder
            for file in os.listdir(new_dir):
                if file.endswith('.csv'):
                    sample = {}
                    sample['pcd_path'] = new_dir/file
                    sample['category'] = category
                    self.files.append(sample)

    def __len__(self):
        return len(self.files)

    def __preproc__(self, file):
        #verts, faces = read_off(file)
        std_pointcloud = Standardize(file,self.features,self.persistence).ptcld
        if self.transforms:
            pointcloud = self.transforms(std_pointcloud)
        return pointcloud

    def __getitem__(self, idx):
        pcd_path = self.files[idx]['pcd_path']
        category = self.files[idx]['category']
        #with open(pcd_path, 'r') as f:
        pointcloud = self.__preproc__(pcd_path)
        return {'pointcloud': pointcloud,
                'category': self.classes[category]}

if __name__ == '__main__': # Execute the code only from this program not when Pointnet is called externally

    train_transforms = transforms.Compose([
                        RandomNoise(),
                        ToTensor()
                        ])
    
    train_ds = PointCloudData(path, features, persistence, transform=train_transforms)
    valid_ds = PointCloudData(path, features, persistence, valid=True, folder='valid', transform=train_transforms)

    inv_classes = {i: cat for cat, i in train_ds.classes.items()};
    inv_classes
    
    print('Train dataset size: ', len(train_ds))
    print('Valid dataset size: ', len(valid_ds))
    print('Number of classes: ', len(train_ds.classes))
    print('Sample pointcloud shape: ', train_ds[0]['pointcloud'].size())
    print('Class: ', inv_classes[train_ds[0]['category']])

    train_loader = DataLoader(dataset=train_ds, batch_size=64, shuffle=True)
    valid_loader = DataLoader(dataset=valid_ds, batch_size=64)

#%%

"""## Model"""


class Transform(nn.Module):
    def __init__(self):
        super().__init__()

        # Shared MLP for all points in the point cloud
        self.conv1 = nn.Conv1d(features, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 512, 1)

        # Batch normalization
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(512)

    def forward(self, input):
        """
        Forward pass for the feature transform network and the input transform network for the point cloud input
        :param input: torch tensor of dimensions [bs, n, k], where n is the number of points in the point cloud and k
        is the number of features
        :return: global feature vector, matrix for input transformation, matrix for feature transformation
        """

        # Input passed through shared MLP for all points
        xb = self.conv1(input)
        xb = self.bn1(xb)
        xb = F.relu(xb)

        # Data passed through 2nd shared MLP for all points
        xb = self.conv2(xb)
        xb = self.bn2(xb)
        xb = F.relu(xb)

        # Passed through 3rd shared MLP for all points and global max pooling
        # Flatten the output to get the global feature vector
        xb = self.conv3(xb)
        xb = self.bn3(xb)

        xb = nn.MaxPool1d(xb.size(-1))(xb)
        xb = xb.to(device)

        xb = nn.Flatten(1)(xb)
        xb = xb.to(device)

        return xb


class PointNet(nn.Module):
    def __init__(self, classes=6):
        super().__init__()
        self.transform = Transform().to(device)

        self.fc1 = nn.Linear(512, 128).to(device)
        self.fc2 = nn.Linear(128, classes).to(device)

        self.relu = nn.ReLU()
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout = nn.Dropout(p=0.3)

        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, input):

        # Get the global feature vector
        global_feature = self.transform(input)

        out = self.relu(global_feature)
        out = self.dropout(out)
        out = self.fc1(out)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)

        out = self.logsoftmax(out)

        return out

def pointnetloss(outputs, labels):
    criterion = torch.nn.NLLLoss()
    return criterion(outputs, labels) # only outputs and labels used for defining loss since TNET not being used

#%%
"""## Training loop

"""

training_losses = []
validation_losses = []
training_accuracies = []
validation_accuracies = []

if __name__ == '__main__': # Execute the code only from this program not when Pointnet is called externally

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    no_of_epochs = 30
    pointnet = PointNet()
    pointnet = pointnet.to(device)
    summary(pointnet, (64, 4, 1024), device='cuda')

    optimizer = torch.optim.Adam(list(pointnet.parameters()), lr=0.001)

    def train(model, train_loader, val_loader=None,  epochs=no_of_epochs, save=True):
        for epoch in range(epochs):
            print('Epoch: %d' % epoch)
            pointnet.train()
            running_training_loss = 0.0
            correct_train = total_train = 0

            for i, data in enumerate(train_loader, 0):
                inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)

                optimizer.zero_grad()
                outputs = pointnet(inputs.transpose(1, 2)).to(device)

                loss = pointnetloss(outputs, labels).to(device)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(pointnet.parameters(), max_norm=1.0)
                optimizer.step()

                # print statistics
                # Accumulate training loss
                running_training_loss += loss.item()

                # Calculate training accuracy
                _, predicted = torch.max(outputs.data, 1)
                total_train += labels.size(0)
                correct_train += (predicted == labels).sum().item()

            training_loss = running_training_loss / len(train_loader)
            training_accuracy = 100. * correct_train / total_train

            # Append training loss and accuracy to lists
            training_losses.append(training_loss)
            training_accuracies.append(training_accuracy)

            print('Epoch: %d, Training loss: %.3f, Training accuracy: %.2f%%' %
                  (epoch, training_loss, training_accuracy))

            pointnet.eval()
            running_val_loss = 0.0
            correct_val = total_val = 0
            # validation
            if val_loader:
                with torch.no_grad():
                    for data in val_loader:
                        inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)
                        outputs = pointnet(inputs.transpose(1,2))
                        outputs = outputs.to(device)

                        loss = pointnetloss(outputs, labels).to(device)
                        running_val_loss += loss.item()

                        _, predicted = torch.max(outputs.data, 1)
                        total_val += labels.size(0)
                        correct_val += (predicted == labels).sum().item()

                # Calculate average validation loss and accuracy
                val_loss = running_val_loss / len(val_loader)
                val_accuracy = 100. * correct_val / total_val

                # Append validation loss and accuracy to lists
                validation_losses.append(val_loss)
                validation_accuracies.append(val_accuracy)

                print('Epoch: %d, Validation loss: %.3f, Validation accuracy: %.2f%%' %
                      (epoch, val_loss, val_accuracy))

            # Save the model
            if save and epoch == no_of_epochs - 1:
                torch.save(pointnet.state_dict(),
                           "POINTNET_save_RF" + str(features) + "P" + str(persistence) + "_" + str(epoch) + ".pth")

    # Start timer
    start = time.time()

    if enable_training:
        train(pointnet, train_loader, valid_loader, save=True)

    # End timer
    end = time.time()
    time = end - start
    print("Time taken for training:", time)

    # Plotting training and validation loss
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', marker='o')
    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    # Make x-axis integers from 1 to epoch number + 1
    plt.xticks([10, 20, 30, 40, 50])

    # Plotting training and validation accuracy
    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(training_accuracies) + 1), training_accuracies, label='Training Accuracy', marker='o')
    plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, label='Validation Accuracy',
             marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True)
    plt.xticks([10, 20, 30, 40, 50])
    plt.savefig("POINTNET_F" + str(features) + "P" + str(persistence) + "_loss_&_accuracy.pdf")

    plt.tight_layout()
    plt.show()

#%%

"""## Test"""
if __name__ == '__main__': # Execute the code only from this program not when Pointnet is called externally

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    epochno = no_of_epochs - 1

    model_name = "POINTNET_save_RF" + str(features) + "P" + str(persistence) + "_" + str(epochno) + ".pth"
    figname_norm = "POINTNET_F" + str(features) + "P" + str(persistence) + "_" + str(epochno) + "norm.pdf"
    figname_wn = "POINTNET_F" + str(features) + "P" + str(persistence) + "_" + str(epochno) + "WN.pdf"

    test_ds = PointCloudData(path, features, persistence, valid=True, folder='test', transform=default_transforms)
    test_loader = DataLoader(dataset=test_ds, batch_size=64)

    pointnet = PointNet()
    pointnet = pointnet.to(device)
    pointnet.load_state_dict(torch.load(model_name))
    pointnet.eval()
    
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for i, data in enumerate(test_loader):
            print('Batch [%4d / %4d]' % (i+1, len(test_loader)))
    
            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)
            outputs = pointnet(inputs.transpose(1, 2))
            outputs = outputs.to(device)

            _, preds = torch.max(outputs.data, 1)
            all_preds += list(preds.cpu().numpy())
            all_labels += list(labels.cpu().numpy())
    
    cm = confusion_matrix(all_labels, all_preds)

    # function from https://deeplizard.com/learn/video/0LhiS6yu2qQ
    def plot_confusion_matrix(cm, classes, normalize=False,
                              title='Confusion matrix. ' + str(round(accuracy_score(all_labels, all_preds), 3)),
                              cmap=plt.cm.Blues):
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print("Normalized confusion matrix")
        else:
            print('Confusion matrix, without normalization')

        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title, size=48)

        cbar = plt.colorbar(aspect=10)  # Add the colorbar here
        cbar.ax.tick_params(labelsize=36)

        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45, size=36)
        plt.yticks(tick_marks, classes, size=36)

        fmt = '.2f' if normalize else 'd'
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fmt),
                     horizontalalignment="center", size=36, color="white" if cm[i, j] > thresh else "black")

        plt.ylabel('True label', size=42)
        plt.xlabel('Predicted label', size=42)


    plt.figure(figsize=(24, 24))
    plot_confusion_matrix(cm, list(classes.keys()), normalize=True)
    plt.savefig(figname_norm, bbox_inches='tight')
    plt.show()
    plt.figure(figsize=(24, 24))
    plot_confusion_matrix(cm, list(classes.keys()), normalize=False)
    plt.savefig(figname_wn, bbox_inches='tight')
    plt.show()

    print("Test metrics:")
    print('F1 score:%.4f ' % f1_score(all_labels, all_preds, average='weighted'))
    print('Accuracy score:%.4f' % accuracy_score(all_labels, all_preds))
    print('Precision score:%.4f ' % precision_score(all_labels, all_preds, average='weighted'))
    print('Recall score:%.4f' % recall_score(all_labels, all_preds, average='weighted'))
    print('Time taken for training:', time)